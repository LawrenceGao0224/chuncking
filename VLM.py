import os
import requests
from pathlib import Path
from typing import List, Dict, Optional
import json
from pdf2image import convert_from_path
import re
from io import BytesIO

class PDFToMarkdownPipeline:
    """
    æ”¹é€²ç‰ˆ PDF â†’ Image â†’ VLM â†’ Markdown â†’ Chunked Blocks
    
    æ”¹é€²è¦é»ï¼š
    1. ä¸ä½¿ç”¨ Base64ï¼ˆç›´æ¥å‚³é€åœ–ç‰‡æª”æ¡ˆï¼‰
    2. å…ˆåˆä½µæ‰€æœ‰é é¢çš„ Markdownï¼Œå†é€²è¡Œå…¨å±€ Chunkingï¼ˆé¿å…è·¨é åˆ‡åˆ†ï¼‰
    3. è¼¸å‡ºåŒ…å«å®Œæ•´çš„ Chunking å…§å®¹
    """
    
    def __init__(
        self,
        vlm_model: str = "llama3.2-vision",
        vlm_api_url: str = "http://localhost:11434/api/generate",
        output_dpi: int = 300
    ):
        self.vlm_model = vlm_model
        self.vlm_api_url = vlm_api_url
        self.output_dpi = output_dpi
    
    def pdf_to_images(self, pdf_path: str, temp_dir: str = "./temp_images") -> List[str]:
        """
        Step 1: PDF è½‰åœ–ç‰‡ä¸¦ä¿å­˜åˆ°è‡¨æ™‚ç›®éŒ„
        
        Returns:
            åœ–ç‰‡æª”æ¡ˆè·¯å¾‘åˆ—è¡¨
        """
        print(f"[1] å°‡ PDF è½‰æ›ç‚ºåœ–ç‰‡: {pdf_path}")
        
        Path(temp_dir).mkdir(parents=True, exist_ok=True)
        
        images = convert_from_path(
            pdf_path,
            dpi=self.output_dpi,
            fmt='png'
        )
        
        image_paths = []
        for i, img in enumerate(images):
            img_path = Path(temp_dir) / f"page_{i+1}.png"
            img.save(img_path, format='PNG')
            image_paths.append(str(img_path))
            print(f"  âœ“ ç¬¬ {i+1}/{len(images)} é å·²ä¿å­˜: {img_path}")
        
        return image_paths
    
    def call_vlm(self, image_path: str) -> str:
        """
        Step 2: èª¿ç”¨ VLM æ¨è«–ï¼ˆç›´æ¥å‚³é€åœ–ç‰‡æª”æ¡ˆï¼‰
        
        Args:
            image_path: åœ–ç‰‡æª”æ¡ˆè·¯å¾‘
            
        Returns:
            Markdown æ ¼å¼çš„æ–‡æœ¬
        """
        prompt = """è«‹å°‡é€™å¼µåœ–ç‰‡çš„å…§å®¹è½‰æ›ç‚º Markdown æ ¼å¼ã€‚

è¦æ±‚ï¼š
1. ä¿ç•™æ‰€æœ‰æ¨™é¡Œå±¤ç´šï¼ˆä½¿ç”¨ #, ##, ### ç­‰ï¼‰
2. å°‡è¡¨æ ¼è½‰æ›ç‚º Markdown è¡¨æ ¼èªæ³•ï¼ˆ|---|ï¼‰
3. ä¿ç•™åˆ—è¡¨çµæ§‹ï¼ˆä½¿ç”¨ - æˆ– 1. 2. 3.ï¼‰
4. å¿½ç•¥é ç¢¼ã€é é¦–ã€é å°¾
5. å¦‚æœå…§å®¹çœ‹èµ·ä¾†åœ¨é é¢é‚Šç·£è¢«æˆªæ–·ï¼Œè«‹æ¨™è¨˜ [ç¹¼çºŒæ–¼ä¸‹ä¸€é ]

ä¸è¦æ·»åŠ ä»»ä½•è§£é‡‹ï¼Œç›´æ¥è¼¸å‡º Markdown å…§å®¹ã€‚"""
        
        if "localhost" in self.vlm_api_url or "127.0.0.1" in self.vlm_api_url:
            return self._call_ollama_vlm(image_path, prompt)
        else:
            return self._call_remote_vlm(image_path, prompt)
    
    def _call_ollama_vlm(self, image_path: str, prompt: str) -> str:
        """èª¿ç”¨ Ollama æœ¬åœ° VLMï¼ˆç›´æ¥ä½¿ç”¨æª”æ¡ˆè·¯å¾‘ï¼‰"""
        
        # è®€å–åœ–ç‰‡ç‚ºäºŒé€²åˆ¶æ•¸æ“š
        with open(image_path, 'rb') as f:
            image_data = f.read()
        
        # Ollama éœ€è¦ base64ï¼Œä½†æˆ‘å€‘åœ¨é€™è£¡è™•ç†ï¼Œä¸æš´éœ²çµ¦å¤–éƒ¨
        import base64
        image_b64 = base64.b64encode(image_data).decode('utf-8')
        
        payload = {
            "model": self.vlm_model,
            "prompt": prompt,
            "images": [image_b64],
            "stream": False
        }
        
        try:
            response = requests.post(
                self.vlm_api_url,
                json=payload,
                timeout=300
            )
            response.raise_for_status()
            return response.json().get('response', '')
        except Exception as e:
            print(f"  âœ— VLM æ¨è«–å¤±æ•—: {e}")
            return ""
    
    def _call_remote_vlm(self, image_path: str, prompt: str) -> str:
        """
        èª¿ç”¨é ç«¯ VLM APIï¼ˆä½¿ç”¨ multipart/form-data ä¸Šå‚³åœ–ç‰‡ï¼‰
        é©ç”¨æ–¼æ”¯æ´æª”æ¡ˆä¸Šå‚³çš„ API
        """
        api_key = os.getenv("VLM_API_KEY", "")
        if not api_key:
            print("  âœ— ç¼ºå°‘ VLM_API_KEY ç’°å¢ƒè®Šæ•¸")
            return ""
        
        headers = {
            "Authorization": f"Bearer {api_key}"
        }
        
        with open(image_path, 'rb') as f:
            files = {'image': f}
            data = {
                'model': self.vlm_model,
                'prompt': prompt
            }
            
            try:
                response = requests.post(
                    self.vlm_api_url,
                    headers=headers,
                    files=files,
                    data=data,
                    timeout=300
                )
                response.raise_for_status()
                return response.json().get('content', '')
            except Exception as e:
                print(f"  âœ— é ç«¯ VLM æ¨è«–å¤±æ•—: {e}")
                return ""
    
    def merge_cross_page_content(self, page_markdowns: List[str]) -> str:
        """
        åˆä½µè·¨é å…§å®¹
        
        è™•ç†é‚è¼¯ï¼š
        1. æª¢æ¸¬é é¢çµå°¾çš„ [ç¹¼çºŒæ–¼ä¸‹ä¸€é ] æ¨™è¨˜
        2. ç§»é™¤é‡è¤‡çš„æ¨™é¡Œ
        3. åˆä½µè¢«æˆªæ–·çš„æ®µè½
        """
        if not page_markdowns:
            return ""
        
        merged = []
        
        for i, md in enumerate(page_markdowns):
            lines = md.strip().split('\n')
            
            # ç§»é™¤ [ç¹¼çºŒæ–¼ä¸‹ä¸€é ] æ¨™è¨˜
            cleaned_lines = [
                line for line in lines 
                if '[ç¹¼çºŒæ–¼ä¸‹ä¸€é ]' not in line and '[continued]' not in line.lower()
            ]
            
            # å¦‚æœæ˜¯æœ€å¾Œä¸€é ï¼Œç›´æ¥æ·»åŠ 
            if i == len(page_markdowns) - 1:
                merged.extend(cleaned_lines)
            else:
                # æª¢æŸ¥æ˜¯å¦æœ‰æœªå®Œæˆçš„æ®µè½ï¼ˆæœ€å¾Œä¸€è¡Œä¸æ˜¯ç©ºè¡Œã€æ¨™é¡Œæˆ–åˆ—è¡¨ï¼‰
                if cleaned_lines and not re.match(r'^(#{1,6}\s|[-*]\s|\d+\.\s|$)', cleaned_lines[-1]):
                    # æ¨™è¨˜ç‚ºæœªå®Œæˆï¼Œä¸‹ä¸€é å¯èƒ½æ¥çºŒ
                    merged.extend(cleaned_lines)
                    merged.append("")  # æ·»åŠ åˆ†éš”ç¬¦
                else:
                    merged.extend(cleaned_lines)
                    merged.append("")
        
        return '\n'.join(merged)
    
    def split_markdown_by_headers(
        self,
        markdown_text: str,
        min_chunk_size: int = 100,
        max_chunk_size: int = 2000
    ) -> List[Dict[str, any]]:
        """
        Step 3: å…¨å±€ Header-based Markdown Chunking
        
        æ”¹é€²ç­–ç•¥ï¼š
        1. æŒ‰ç…§æ¨™é¡Œå±¤ç´šåˆ‡åˆ†ï¼ˆ# > ## > ###ï¼‰
        2. ä¿æŒæ¯å€‹ chunk çš„èªæ„å®Œæ•´æ€§
        3. æ”¯æ´æœ€å°/æœ€å¤§ chunk å¤§å°æ§åˆ¶
        
        Returns:
            Chunk åˆ—è¡¨ï¼Œæ¯å€‹åŒ…å«ï¼š
            {
                "chunk_id": int,
                "level": int,
                "title": str,
                "content": str,
                "word_count": int,
                "headers_path": List[str]  # æ¨™é¡Œè·¯å¾‘ï¼ˆéºµåŒ…å±‘ï¼‰
            }
        """
        chunks = []
        lines = markdown_text.split('\n')
        
        current_chunk = {
            "level": 0,
            "title": "Document Start",
            "content_lines": [],
            "headers_path": []
        }
        
        headers_stack = []  # ç”¨æ–¼è¿½è¹¤æ¨™é¡Œå±¤ç´šè·¯å¾‘
        
        for line in lines:
            # æª¢æ¸¬æ¨™é¡Œ
            match = re.match(r'^(#{1,6})\s+(.+)$', line)
            
            if match:
                level = len(match.group(1))
                title = match.group(2).strip()
                
                # ä¿å­˜èˆŠ chunkï¼ˆå¦‚æœæœ‰å…§å®¹ï¼‰
                if current_chunk["content_lines"]:
                    content = '\n'.join(current_chunk["content_lines"]).strip()
                    if len(content) >= min_chunk_size or current_chunk["level"] <= 2:
                        chunks.append({
                            "chunk_id": len(chunks) + 1,
                            "level": current_chunk["level"],
                            "title": current_chunk["title"],
                            "content": content,
                            "word_count": len(content),
                            "headers_path": current_chunk["headers_path"].copy()
                        })
                
                # æ›´æ–°æ¨™é¡Œè·¯å¾‘
                while headers_stack and headers_stack[-1]["level"] >= level:
                    headers_stack.pop()
                
                headers_stack.append({"level": level, "title": title})
                headers_path = [h["title"] for h in headers_stack]
                
                # é–‹å§‹æ–° chunk
                current_chunk = {
                    "level": level,
                    "title": title,
                    "content_lines": [line],
                    "headers_path": headers_path
                }
            else:
                current_chunk["content_lines"].append(line)
        
        # ä¿å­˜æœ€å¾Œä¸€å€‹ chunk
        if current_chunk["content_lines"]:
            content = '\n'.join(current_chunk["content_lines"]).strip()
            if content:
                chunks.append({
                    "chunk_id": len(chunks) + 1,
                    "level": current_chunk["level"],
                    "title": current_chunk["title"],
                    "content": content,
                    "word_count": len(content),
                    "headers_path": current_chunk["headers_path"].copy()
                })
        
        # åˆä½µéå°çš„ chunks
        chunks = self._merge_small_chunks(chunks, min_chunk_size, max_chunk_size)
        
        return chunks
    
    def _merge_small_chunks(
        self,
        chunks: List[Dict],
        min_size: int,
        max_size: int
    ) -> List[Dict]:
        """åˆä½µéå°çš„ chunksï¼Œé¿å…ç¢ç‰‡åŒ–"""
        if not chunks:
            return []
        
        merged = []
        buffer = None
        
        for chunk in chunks:
            size = chunk["word_count"]
            
            # å¦‚æœ chunk å¤ªå°ä¸”ä¸æ˜¯é ‚ç´šæ¨™é¡Œ
            if size < min_size and chunk["level"] > 2:
                if buffer is None:
                    buffer = chunk
                else:
                    # åˆä½µåˆ° buffer
                    buffer["content"] += "\n\n" + chunk["content"]
                    buffer["word_count"] += chunk["word_count"]
                    buffer["title"] += " + " + chunk["title"]
            else:
                # å…ˆæ¸…ç©º buffer
                if buffer:
                    merged.append(buffer)
                    buffer = None
                
                # å¦‚æœ chunk å¤ªå¤§ï¼Œå˜—è©¦æ‹†åˆ†
                if size > max_size:
                    split_chunks = self._split_large_chunk(chunk, max_size)
                    merged.extend(split_chunks)
                else:
                    merged.append(chunk)
        
        # è™•ç†å‰©é¤˜çš„ buffer
        if buffer:
            merged.append(buffer)
        
        # é‡æ–°ç·¨è™Ÿ
        for i, chunk in enumerate(merged, 1):
            chunk["chunk_id"] = i
        
        return merged
    
    def _split_large_chunk(self, chunk: Dict, max_size: int) -> List[Dict]:
        """å°‡éå¤§çš„ chunk æŒ‰æ®µè½æ‹†åˆ†"""
        paragraphs = chunk["content"].split('\n\n')
        sub_chunks = []
        current_content = []
        current_size = 0
        
        for para in paragraphs:
            para_size = len(para)
            
            if current_size + para_size > max_size and current_content:
                # ä¿å­˜ç•¶å‰ sub-chunk
                sub_chunks.append({
                    "chunk_id": 0,  # ç¨å¾Œé‡æ–°ç·¨è™Ÿ
                    "level": chunk["level"],
                    "title": f"{chunk['title']} (Part {len(sub_chunks) + 1})",
                    "content": '\n\n'.join(current_content),
                    "word_count": current_size,
                    "headers_path": chunk["headers_path"]
                })
                current_content = [para]
                current_size = para_size
            else:
                current_content.append(para)
                current_size += para_size
        
        # ä¿å­˜æœ€å¾Œä¸€å€‹ sub-chunk
        if current_content:
            sub_chunks.append({
                "chunk_id": 0,
                "level": chunk["level"],
                "title": f"{chunk['title']} (Part {len(sub_chunks) + 1})" if sub_chunks else chunk['title'],
                "content": '\n\n'.join(current_content),
                "word_count": current_size,
                "headers_path": chunk["headers_path"]
            })
        
        return sub_chunks if sub_chunks else [chunk]
    
    def process_pdf(
        self,
        pdf_path: str,
        output_dir: Optional[str] = None,
        keep_temp_images: bool = False
    ) -> Dict:
        """
        åŸ·è¡Œå®Œæ•´ Pipeline
        
        æµç¨‹ï¼š
        1. PDF â†’ Images
        2. Images â†’ Markdown (é€é )
        3. åˆä½µæ‰€æœ‰ Markdown (è™•ç†è·¨é )
        4. å…¨å±€ Chunking (é¿å…è·¨é åˆ‡åˆ†)
        
        Returns:
            {
                "full_markdown": str,
                "chunks": [...],
                "metadata": {...}
            }
        """
        print("\n" + "="*70)
        print("ğŸš€ é–‹å§‹ PDF â†’ Markdown â†’ Chunks Pipeline (æ”¹é€²ç‰ˆ)")
        print("="*70 + "\n")
        
        temp_dir = "./temp_images"
        
        # Step 1: PDF â†’ Images
        image_paths = self.pdf_to_images(pdf_path, temp_dir)
        
        # Step 2: Images â†’ Markdown (é€é è™•ç†)
        print(f"\n[2] VLM æ¨è«–ä¸­...")
        page_markdowns = []
        
        for i, img_path in enumerate(image_paths, 1):
            print(f"  è™•ç†ç¬¬ {i}/{len(image_paths)} é ...")
            markdown = self.call_vlm(img_path)
            
            if markdown:
                page_markdowns.append(markdown)
                print(f"    âœ“ å®Œæˆ ({len(markdown)} å­—å…ƒ)")
            else:
                print(f"    âœ— å¤±æ•—")
        
        # Step 3: åˆä½µè·¨é å…§å®¹
        print(f"\n[3] åˆä½µ {len(page_markdowns)} é çš„ Markdown...")
        full_markdown = self.merge_cross_page_content(page_markdowns)
        print(f"  âœ“ åˆä½µå®Œæˆ (ç¸½è¨ˆ {len(full_markdown)} å­—å…ƒ)")
        
        # Step 4: å…¨å±€ Chunking
        print(f"\n[4] åŸ·è¡Œå…¨å±€èªæ„åˆ†å¡Š...")
        chunks = self.split_markdown_by_headers(
            full_markdown,
            min_chunk_size=100,
            max_chunk_size=2000
        )
        print(f"  âœ“ ç”Ÿæˆ {len(chunks)} å€‹èªæ„ Chunks")
        
        # æ§‹å»ºçµæœ
        results = {
            "full_markdown": full_markdown,
            "chunks": chunks,
            "metadata": {
                "pdf_path": pdf_path,
                "total_pages": len(image_paths),
                "total_chunks": len(chunks),
                "model": self.vlm_model,
                "dpi": self.output_dpi,
                "avg_chunk_size": sum(c["word_count"] for c in chunks) // len(chunks) if chunks else 0
            }
        }
        
        # ä¿å­˜è¼¸å‡º
        if output_dir:
            self._save_results(pdf_path, results, output_dir)
        
        # æ¸…ç†è‡¨æ™‚åœ–ç‰‡
        if not keep_temp_images:
            import shutil
            shutil.rmtree(temp_dir, ignore_errors=True)
            print(f"\nğŸ§¹ å·²æ¸…ç†è‡¨æ™‚åœ–ç‰‡")
        
        print(f"\n" + "="*70)
        print(f"âœ… å®Œæˆ!")
        print(f"   â€¢ ç¸½é æ•¸: {results['metadata']['total_pages']}")
        print(f"   â€¢ Chunks: {results['metadata']['total_chunks']}")
        print(f"   â€¢ å¹³å‡å¤§å°: {results['metadata']['avg_chunk_size']} å­—å…ƒ/chunk")
        print("="*70 + "\n")
        
        return results
    
    def _save_results(self, pdf_path: str, results: Dict, output_dir: str):
        """ä¿å­˜çµæœ"""
        Path(output_dir).mkdir(parents=True, exist_ok=True)
        pdf_name = Path(pdf_path).stem
        
        # 1. ä¿å­˜å®Œæ•´ Markdown
        md_path = Path(output_dir) / f"{pdf_name}_full.md"
        with open(md_path, 'w', encoding='utf-8') as f:
            f.write(results["full_markdown"])
        print(f"  ğŸ’¾ å®Œæ•´ Markdown: {md_path}")
        
        # 2. ä¿å­˜ Chunks (JSON)
        chunks_json_path = Path(output_dir) / f"{pdf_name}_chunks.json"
        with open(chunks_json_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        print(f"  ğŸ’¾ Chunks JSON: {chunks_json_path}")
        
        # 3. ä¿å­˜ Chunks (Markdown æ ¼å¼ï¼Œæ–¹ä¾¿é–±è®€)
        chunks_md_path = Path(output_dir) / f"{pdf_name}_chunks.md"
        with open(chunks_md_path, 'w', encoding='utf-8') as f:
            f.write(f"# æ–‡æª”åˆ†å¡Šçµæœ\n\n")
            f.write(f"**ä¾†æº:** {pdf_path}\n")
            f.write(f"**ç¸½ Chunks:** {len(results['chunks'])}\n\n")
            f.write("---\n\n")
            
            for chunk in results["chunks"]:
                f.write(f"## Chunk {chunk['chunk_id']}: {chunk['title']}\n\n")
                f.write(f"**å±¤ç´š:** {chunk['level']} | ")
                f.write(f"**å­—æ•¸:** {chunk['word_count']} | ")
                f.write(f"**è·¯å¾‘:** {' > '.join(chunk['headers_path'])}\n\n")
                f.write("```\n")
                f.write(chunk['content'][:500])  # é è¦½å‰ 500 å­—å…ƒ
                if len(chunk['content']) > 500:
                    f.write("\n... (ç•¥)")
                f.write("\n```\n\n")
                f.write("---\n\n")
        
        print(f"  ğŸ’¾ Chunks Markdown: {chunks_md_path}")


# ============================================================================
# ä½¿ç”¨ç¤ºä¾‹
# ============================================================================

if __name__ == "__main__":
    # åˆå§‹åŒ– Pipeline
    pipeline = PDFToMarkdownPipeline(
        vlm_model="llama3.2-vision",  # ä½¿ç”¨ Llama 3.2 Vision
        vlm_api_url="http://localhost:11434/api/generate",
        output_dpi=300
    )
    
    # åŸ·è¡Œè™•ç†
    results = pipeline.process_pdf(
        pdf_path="OCR.pdf",
        output_dir="./output",
        keep_temp_images=False  # è™•ç†å®Œæˆå¾Œåˆªé™¤è‡¨æ™‚åœ–ç‰‡
    )
    
    # æŸ¥çœ‹çµæœçµ±è¨ˆ
    print("\nğŸ“Š çµæœçµ±è¨ˆ:")
    print(f"  â€¢ Markdown ç¸½é•·åº¦: {len(results['full_markdown'])} å­—å…ƒ")
    print(f"  â€¢ Chunks æ•¸é‡: {len(results['chunks'])}")
    print(f"\nğŸ“„ å‰ 3 å€‹ Chunks:")
    
    for chunk in results["chunks"][:3]:
        print(f"\n  [{chunk['chunk_id']}] {chunk['title']}")
        print(f"      è·¯å¾‘: {' > '.join(chunk['headers_path'])}")
        print(f"      å…§å®¹: {chunk['content'][:100]}...")